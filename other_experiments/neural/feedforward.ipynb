{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Neural model for classification"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Libraries import"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-16T14:37:33.454577Z","iopub.status.busy":"2023-06-16T14:37:33.453185Z","iopub.status.idle":"2023-06-16T14:37:41.461212Z","shell.execute_reply":"2023-06-16T14:37:41.459455Z","shell.execute_reply.started":"2023-06-16T14:37:33.454538Z"},"trusted":true},"outputs":[],"source":["import os\n","import pickle\n","from typing import Callable, Dict, List, Set, Tuple\n","\n","import matplotlib.pyplot as plt\n","import numpy as np  # linear algebra\n","import pandas as pd  # data processing, CSV file I/O (e.g. pd.read_csv)\n","import seaborn as sns\n","from dotenv import load_dotenv\n","\n","from tqdm.notebook import tqdm \n","import matplotlib.pyplot as plt\n","from pathlib import Path\n","import random\n","import sklearn.preprocessing\n","import category_encoders as ce\n","\n","import optuna\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader\n","from torch.profiler import profile, record_function, ProfilerActivity"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-16T14:37:45.396011Z","iopub.status.busy":"2023-06-16T14:37:45.395497Z","iopub.status.idle":"2023-06-16T14:37:45.572537Z","shell.execute_reply":"2023-06-16T14:37:45.571061Z","shell.execute_reply.started":"2023-06-16T14:37:45.395968Z"},"trusted":true},"outputs":[],"source":["from sklearn.metrics import classification_report\n","from sklearn.metrics.pairwise import cosine_similarity\n","from sklearn.preprocessing import MinMaxScaler, StandardScaler\n","from utils.preprocessing import labelEncodeCats, remove_categories_not_in_both, remove_outliers, CATEGORICAL_TO_DROP, NUMERICAL_NON_COUNTERS, NUMERICAL_TO_DROP\n","from utils.normalized_cross_entropy_loss import normalized_cross_entropy_loss\n","from utils.preprocessing import (\n","    encode_counters,\n","    remove_categories_not_in_both,\n","    remove_outliers,\n","    trigonometric_date_encoding,\n",")"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Random seed"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-16T14:37:45.578171Z","iopub.status.busy":"2023-06-16T14:37:45.577720Z","iopub.status.idle":"2023-06-16T14:37:45.585664Z","shell.execute_reply":"2023-06-16T14:37:45.584039Z","shell.execute_reply.started":"2023-06-16T14:37:45.578128Z"},"trusted":true},"outputs":[],"source":["seed = 1234\n","\n","np.random.seed(seed)\n","random.seed(seed)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Dataset loading"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-16T14:37:45.606989Z","iopub.status.busy":"2023-06-16T14:37:45.606547Z","iopub.status.idle":"2023-06-16T14:37:56.038120Z","shell.execute_reply":"2023-06-16T14:37:56.036504Z","shell.execute_reply.started":"2023-06-16T14:37:45.606954Z"},"trusted":true},"outputs":[],"source":["TRAIN_VAL_DATA_PATH: Path = os.path.join('data/', 'train_val_Enc_Counters.parquet')\n","TEST_DATA_PATH: Path = os.path.join('data/', 'test_val_Enc_Counters.parquet')\n","\n","df = pd.read_parquet(TRAIN_VAL_DATA_PATH).reset_index(drop=True)\n","\n","df = df.astype({f\"f_{i}\": \"category\" for i in range(2, 33)})\n","df = df.astype({\"f_1\": \"int\"})\n","\n","df = df.astype({\"is_clicked\": \"int\"})\n","df = df.astype({\"is_installed\": \"int\"})\n","\n","df_test = pd.read_parquet(TEST_DATA_PATH).reset_index(drop=True)\n","df_test = df_test.astype({f\"f_{i}\": \"category\" for i in range(2, 33)})\n","df_test = df_test.astype({\"f_1\": \"int\"})"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-16T14:37:56.040169Z","iopub.status.busy":"2023-06-16T14:37:56.039789Z","iopub.status.idle":"2023-06-16T14:37:56.090572Z","shell.execute_reply":"2023-06-16T14:37:56.088823Z","shell.execute_reply.started":"2023-06-16T14:37:56.040140Z"},"trusted":true},"outputs":[],"source":["boolean_columns: List[str] = [f\"f_{i}\" for i in range(33, 42)]\n","\n","# convert boolean columns to bool otherwise catboost will throw an error\n","for col in boolean_columns:\n","    df[col] = df[col].astype(bool)\n","    df_test[col] = df_test[col].astype(bool)\n","\n","# union usless feaatures with backword selection\n","CATEGORICAL_TO_DROP: list = [\n","    \"f_7\",\n","    \"f_9\",\n","    \"f_11\",\n","    \"f_23\",\n","    \"f_24\",\n","    \"f_25\",\n","    \"f_26\",\n","    \"f_27\",\n","    \"f_28\",\n","    \"f_29\",\n","]\n","\n","NUMERICAL_TO_DROP: list = [\n","    \"f_55\",\n","    \"f_59\",\n","    \"f_64\",\n","    \"f_65\",\n","    \"f_66\",\n","]\n","\n","NUMERICAL_NON_COUNTERS: List[str] = [\n","    \"f_43\",\n","    \"f_51\",\n","    \"f_58\",\n","    \"f_59\",\n","    \"f_64\",\n","    \"f_65\",\n","    \"f_66\",\n","    \"f_67\",\n","    \"f_68\",\n","    \"f_69\",\n","    \"f_70\",\n","]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-16T14:37:56.092978Z","iopub.status.busy":"2023-06-16T14:37:56.092503Z","iopub.status.idle":"2023-06-16T14:37:56.110493Z","shell.execute_reply":"2023-06-16T14:37:56.109317Z","shell.execute_reply.started":"2023-06-16T14:37:56.092941Z"},"trusted":true},"outputs":[],"source":["from utils.notebook_utils import collapse_binary\n","\n","# if do you want to collapse the binary columns set collapse_binary to True\n","activate_collapse_binary = False\n","\n","if activate_collapse_binary:\n","    categorical_columns: List[str] = [f\"f_{i}\" for i in range(2, 32 + 1)] + [\"f_394041\", \"f_33457\"]\n","else:\n","    categorical_columns: List[str] = [f\"f_{i}\" for i in range(2, 32 + 1)]\n","\n","numerical_columns: List[str] = [f\"f_{i}\" for i in range(42, 79 + 1)]\n","categorical_columns = [col for col in categorical_columns if col not in CATEGORICAL_TO_DROP]\n","numerical_columns = [\n","    col\n","    for col in numerical_columns\n","    if col not in NUMERICAL_TO_DROP and col in NUMERICAL_NON_COUNTERS\n","]\n","counter_columns: List[str] = [f\"f_{i}\" for i in range(42, 79 + 1)]\n","counter_columns = [\n","    col\n","    for col in counter_columns\n","    if col not in NUMERICAL_TO_DROP and col not in NUMERICAL_NON_COUNTERS\n","]"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Other models predictions download"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-16T14:37:56.131338Z","iopub.status.busy":"2023-06-16T14:37:56.130436Z","iopub.status.idle":"2023-06-16T14:37:56.142122Z","shell.execute_reply":"2023-06-16T14:37:56.140831Z","shell.execute_reply.started":"2023-06-16T14:37:56.131288Z"},"trusted":true},"outputs":[],"source":["def download_s3_folder(bucket, s3_folder, local_dir=None):\n","    \"\"\"\n","    Download the contents of a folder directory\n","    Args:\n","        bucket_name: the name of the s3 bucket\n","        s3_folder: the folder path in the s3 bucket\n","        local_dir: a relative or absolute directory path in the local file system\n","    \"\"\"\n","    for obj in bucket.objects.filter(Prefix=s3_folder):\n","        target = obj.key if local_dir is None \\\n","            else os.path.join(local_dir, os.path.relpath(obj.key, s3_folder))\n","        if not os.path.exists(os.path.dirname(target)):\n","            os.makedirs(os.path.dirname(target))\n","        if obj.key[-1] == '/':\n","            continue\n","        bucket.download_file(obj.key, target)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["This model can both work as a network on the dataset or as a prediction aggregator (hybrid model)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["intended_as_hybrid = True"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-16T14:37:56.144624Z","iopub.status.busy":"2023-06-16T14:37:56.143885Z"},"trusted":true},"outputs":[],"source":["import boto3\n","from sklearn.preprocessing import OneHotEncoder\n","\n","load_dotenv()\n","aws_access_key_id = os.getenv(\"AWS_ACCESS_KEY_ID\", \"default\")\n","aws_secret_access_key = os.getenv(\"AWS_SECRET_ACCESS_KEY\", \"default\")\n","regiorn_name = \"eu-west-1\"\n","bucket_name = \"challenge23\"\n","\n","s3 = boto3.resource(\n","    \"s3\",\n","    aws_access_key_id=aws_access_key_id,\n","    aws_secret_access_key=aws_secret_access_key,\n","    config=boto3.session.Config(region_name=regiorn_name),\n",")\n","bucket = s3.Bucket(bucket_name)\n","\n","if intended_as_hybrid:\n","\n","    download_s3_folder(bucket, \"hybrid_predictions\", \".\")\n","\n","\n","    # get predictions as df\n","    catboost_cat_preds = pd.read_csv(\"incremental/catboost_categorical_trainval_incremental.csv\", sep=\"\\t\")\n","    catboost_preds = pd.read_csv(\"incremental/catboost_trainval_incremental.csv\", sep=\"\\t\")\n","    catboost_w_preds = pd.read_csv(\"incremental/catboost_weighted_trainval_incremental.csv\", sep=\"\\t\")\n","    light_gbm_preds = pd.read_csv(\"incremental/light_trainval_incremental.csv\", sep=\"\\t\")\n","    light_gbm_clicked_preds = pd.read_csv(\"incremental/light_trainval_is_clicked_incremental.csv\", sep=\"\\t\")\n","    nn_click_preds = pd.read_csv(\"incremental/nn_clickpredictions_trainval_day.csv\", sep=\"\\t\")\n","    nn_preds = pd.read_csv(\"incremental/nn_trainval_incremental.csv\", sep=\"\\t\")\n","    xgb_catasnum_preds = pd.read_csv(\"incremental/xgb_catasnum_trainval_incremental.csv\", sep=\"\\t\")\n","    xgb_num_preds = pd.read_csv(\"incremental/xgboost_numerical_trainval_incremental.csv\", sep=\"\\t\")\n","\n","    clustering_preds = pd.read_csv(\"incremental/kmeans_unsupervised_trainval.csv\", sep=\"\\t\")\n","    one_hot_encoder = OneHotEncoder(handle_unknown='ignore')\n","    encoded_clustering = pd.DataFrame((one_hot_encoder.fit_transform(clustering_preds[[\"kmeans_unsupervised\"]])).toarray())\n","    clustering_preds_new = pd.concat([clustering_preds, encoded_clustering], axis=1)\n","    clustering_preds_new = clustering_preds_new.drop(columns=['kmeans_unsupervised'])\n","\n","    # merging preds\n","    final_dataset = df.merge(catboost_cat_preds, on=\"f_0\")\n","    final_dataset = final_dataset.merge(catboost_preds, on=\"f_0\")\n","    final_dataset = final_dataset.merge(catboost_w_preds, on=\"f_0\")\n","    final_dataset = final_dataset.merge(light_gbm_preds, on=\"f_0\")\n","    final_dataset = final_dataset.merge(light_gbm_clicked_preds, on=\"f_0\")\n","    final_dataset = final_dataset.merge(nn_click_preds, on=\"f_0\")\n","    final_dataset = final_dataset.merge(nn_preds, on=\"f_0\")\n","    final_dataset = final_dataset.merge(xgb_catasnum_preds, on=\"f_0\")\n","    final_dataset = final_dataset.merge(xgb_num_preds, on=\"f_0\")\n","    final_dataset = final_dataset.merge(clustering_preds_new, on=\"f_0\")\n","    final_dataset.columns = final_dataset.columns.astype(str)\n","else:\n","    final_dataset = df\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Preprocessing"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def preprocess_data_nn(\n","    df_train: pd.DataFrame, df_val: pd.DataFrame, Y_train: pd.DataFrame, Y_val: pd.DataFrame\n",") -> Tuple[pd.DataFrame, pd.DataFrame]:\n","    \n","    categorical_columns: List[str] = [f\"f_{i}\" for i in range(2, 32 + 1)]\n","    numerical_columns: List[str] = [f\"f_{i}\" for i in range(42, 79 + 1)]\n","    boolean_columns: List[str] = [f\"f_{i}\" for i in range(33, 42)]\n","    categorical_columns = [col for col in categorical_columns if col not in CATEGORICAL_TO_DROP]\n","    numerical_columns = [col for col in numerical_columns if col not in NUMERICAL_TO_DROP and col in NUMERICAL_NON_COUNTERS]\n","    counter_columns: List[str] = [f\"f_{i}\" for i in range(42, 79 + 1)]\n","    counter_columns = [col for col in counter_columns if col not in NUMERICAL_TO_DROP and col not in NUMERICAL_NON_COUNTERS]\n","    \n","    df_train = df_train.drop(columns=CATEGORICAL_TO_DROP + NUMERICAL_TO_DROP)\n","    df_val = df_val.drop(columns=CATEGORICAL_TO_DROP + NUMERICAL_TO_DROP)\n","\n","    df_train = collapse_binary(df_train, dropOriginal=True)\n","    df_val = collapse_binary(df_val, dropOriginal=True)\n","    \n","    cb_encoder = ce.CatBoostEncoder()\n","    cb_encoder.fit(df_train[categorical_columns], Y_train)\n","    df_train[categorical_columns] = cb_encoder.transform(df_train[categorical_columns])\n","    df_val[categorical_columns] = cb_encoder.transform(df_val[categorical_columns])\n","\n","    df_train, mins_train, steps_train = encode_counters(\n","        df=df_train,\n","        columns=counter_columns,\n","        mins=None,\n","        steps=None,\n","    )\n","    df_val, _, _ = encode_counters(\n","        df=df_val,\n","        columns=counter_columns,\n","        mins=mins_train,\n","        steps=steps_train,\n","    )\n","    counter_modes: pd.Series = df_train[counter_columns].mode()\n","    df_train = df_train.fillna(counter_modes)\n","    df_val = df_val.fillna(counter_modes)\n","    for col in counter_columns:\n","        n_zeros: int = (df_train[col] == 0).sum()\n","        if n_zeros > df_train.shape[0] * 0.95:\n","            df_train[col] = np.where(df_train[col].values, 1, 0)\n","            df_train = df_train.astype({col: \"bool\"})\n","            boolean_columns.append(col)\n","            df_val[col] = np.where(df_val[col].values, 1, 0)\n","            df_val = df_val.astype({col: \"bool\"})\n","        else:\n","            df_train[col] = np.log(df_train[col] + 0.5)\n","            df_val[col] = np.log(df_val[col] + 0.5)\n","\n","    means: pd.Series = df_train[numerical_columns].mean()\n","    stds: pd.Series = df_train[numerical_columns].std()\n","    df_train = remove_outliers(\n","        df=df_train,\n","        columns=numerical_columns,\n","        coefficient=4,\n","        means=means,\n","        stds=stds,\n","    )\n","    df_val = remove_outliers(\n","        df=df_val,\n","        columns=numerical_columns,\n","        coefficient=4,\n","        means=means,\n","        stds=stds,\n","    )\n","\n","    means_no_outliers: pd.Series = df_train[numerical_columns].mean()\n","    stds_no_outliers: pd.Series = df_train[numerical_columns].std()\n","    df_train.loc[:, numerical_columns] = (\n","        df_train.loc[:, numerical_columns] - means_no_outliers\n","    ) / stds_no_outliers\n","    df_val.loc[:, numerical_columns] = (\n","        df_val.loc[:, numerical_columns] - means_no_outliers\n","    ) / stds_no_outliers\n","    df_train = df_train.fillna(means_no_outliers)\n","    df_val = df_val.fillna(means_no_outliers)\n","\n","    scaler = MinMaxScaler()\n","    df_train = scaler.fit_transform(df_train)\n","    df_val = scaler.transform(df_val)\n","    \n","    print(\"Preprocessing ended\")\n","    \n","    return df_train, df_val"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Network structure"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["class Net(nn.Module):\n","    def __init__(self, layer_sizes, activation, dropout_rate, input_features):\n","        super(Net, self).__init__()\n","    \n","        \n","        self.layers = nn.ModuleList([nn.Linear(input_features, layer_sizes[0])])\n","        print(\"layer_sizes \", layer_sizes)\n","        for i in range(len(layer_sizes)-1):\n","            self.layers.append(nn.Linear(layer_sizes[i], layer_sizes[i+1]))\n","        self.activation = activation\n","        self.dropout = nn.Dropout(dropout_rate)\n","        self.output = nn.Linear(layer_sizes[-1], 1)  # Output layer with size 1\n","    \n","  \n","    def forward(self, x):\n","        for layer in self.layers:  # Not applying activation to last layer\n","            x = self.activation(layer(x))\n","            x = self.dropout(x)\n","        x = self.output(x)\n","        return x"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def train(model, X_train, y_train, X_val, y_val, optimizer, criterion, device, trial, num_epochs, batch_size, patience):\n","    \n","    model.train()\n","    \n","    best_val_loss = np.inf\n","    patience_counter = 0\n","    \n","    for epoch in range(num_epochs):\n","        print(f\"Epoch {epoch} started\")\n","        losses = []\n","        batch_counter = 0\n","        # Calculate the number of batches\n","        num_batches = int(len(X_train) / batch_size)\n","\n","        permutation = torch.randperm(X_train.size()[0])\n","        for i in range(0, X_train.size()[0], batch_size):\n","            optimizer.zero_grad()\n","\n","            indices = permutation[i:i+batch_size]\n","            batch_x, batch_y = X_train[indices], y_train[indices]\n","\n","            outputs = torch.sigmoid(model(batch_x))\n","            loss = criterion(outputs, batch_y)\n","            loss.backward()\n","            optimizer.step()\n","\n","            losses.append(loss.item())\n","\n","        # Store the losses in the trial\n","        #trial.set_user_attr('losses', losses)\n","        \n","        # Calculate validation loss\n","        val_loss = evaluate(model, X_val, y_val, criterion, device)\n","        \n","        print(f\"Loss epoch {epoch}: {val_loss}\")\n","        \n","        # Check if we need to save the model\n","        if val_loss < best_val_loss:\n","            best_val_loss = val_loss\n","            patience_counter = 0\n","            torch.save(model.state_dict(), 'best_model.pth')  # Save the model\n","        else:\n","            patience_counter += 1\n","            if patience_counter >= patience:\n","                print(\"Early stopping due to lack of improvement in validation loss.\")\n","                break\n","                \n","        print(f\"Epoch {epoch} finished\")\n","\n","    return best_val_loss"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def evaluate(model, X_val, y_val, criterion, device):\n","    model.eval()\n","    total_loss = 0\n","    total_samples = 0\n","    with torch.no_grad():\n","        outputs = torch.sigmoid(model(X_val))  # Apply sigmoid to outputs\n","        loss = criterion(outputs, y_val)\n","        total_loss += loss.item() * X_val.size(0)\n","        total_samples += X_val.size(0)\n","    return total_loss / total_samples"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["class CustomDataset(torch.utils.data.Dataset):\n","    def __init__(self, data, targets):\n","        self.data = data\n","        self.targets = targets\n","\n","    def __getitem__(self, index):\n","        x = self.data[index]\n","        y = self.targets[index]\n","\n","        return x, y\n","\n","    def __len__(self):\n","        return len(self.data)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Objective function to tune"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def objective(trial):\n","    # Define the search space for hyperparameters\n","    num_layers = trial.suggest_int('num_layers', 1, 7)\n","    layer_sizes = []\n","    for i in range(num_layers):\n","        layer_sizes.append(trial.suggest_int(f'hidden_size_{i}', 16, 256, log=True))\n","    activation_function = trial.suggest_categorical('activation_function', ['relu', 'swish', 'elu'])\n","    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-3, log=True)\n","    weight_decay = trial.suggest_float('weight_decay', 1e-6, 1e-3, log=True)\n","    dropout_rate = trial.suggest_float('dropout_rate', 0.0, 0.3)\n","    batch_size = trial.suggest_categorical('batch_size',[32, 64, 128, 256])\n","\n","    activation_functions = {\n","    'relu': torch.nn.functional.relu,\n","    'elu': torch.nn.functional.elu,\n","    # swish is not a standard function in torch.nn.functional, so we define it\n","    'swish': lambda x: x * torch.sigmoid(x)  \n","    }\n","    \n","    # Create the model and move it to the GPU if available\n","    activation = activation_functions[activation_function]\n","    \n","    # Train the model\n","\n","    val_day = 65\n","        \n","    train_df = final_dataset[final_dataset[\"f_1\"] < val_day]\n","    val_df = final_dataset[final_dataset[\"f_1\"] >= val_day]\n","\n","    X_train = train_df.drop(columns=[\"is_clicked\", \"is_installed\"])\n","    y_train = train_df[[\"is_installed\"]]\n","    X_val = val_df.drop(columns=[\"is_clicked\", \"is_installed\"])\n","    y_val = val_df[[\"is_installed\"]]\n","\n","    X_train, X_val = preprocess_data_nn(X_train, X_val, y_train, y_val)\n","\n","    y_train = y_train.to_numpy()\n","    y_val = y_val.to_numpy()\n","\n","    X_train = torch.tensor(X_train, dtype=torch.float32)\n","    y_train = torch.tensor(y_train, dtype=torch.float32)\n","    X_val = torch.tensor(X_val, dtype=torch.float32)\n","    y_val = torch.tensor(y_val, dtype=torch.float32)\n","\n","    X_train, y_train = X_train.to(device), y_train.to(device)\n","    X_val, y_val = X_val.to(device), y_val.to(device)\n","\n","    input_features = X_train.shape[1]\n","    model = Net(layer_sizes, activation, dropout_rate, input_features).to(device)\n","\n","    # Define the loss function, optimizer, and data loaders\n","    criterion = nn.BCELoss()\n","    optimizer = optim.SGD(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n","    \n","    print(\"Num_epochs: \", num_epochs)\n","    loss = 0\n","    \n","    loss = train(model, X_train, y_train, X_val, y_val, optimizer, criterion, device, trial, num_epochs, batch_size, patience=5)\n","\n","    # Evaluate the model on the validation set\n","    return loss"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","num_epochs = 100"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def optuna_callback(study, trial):\n","    # Access the trial info and study\n","    print(f\"Trial number: {trial.number}\")\n","    print(f\"Trial value (loss): {trial.value}\")\n","    print(f\"Trial parameters: {trial.params}\")\n","    print(f\"Best value so far: {study.best_value}\")\n","    print(f\"Best trial so far: {study.best_trial}\")"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Tuning"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["OPTUNA_STORAGE: str = os.getenv(\"OPTUNA_STORAGE\", \"sqlite://optuna.db\")\n","study = optuna.create_study(direction='minimize', sampler=optuna.samplers.TPESampler(seed=777), study_name=\"Hivemind\", storage=OPTUNA_STORAGE)\n","study.optimize(objective, n_trials=200)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["trial = study.best_trial\n","print('Best loss: ', trial.value)\n","print('Best parameters: ')\n","for key, value in trial.params.items():\n","    print(f\"  {key}: {value}\")"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":4}
